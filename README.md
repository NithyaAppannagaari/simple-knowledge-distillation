# Simple Knowledge Distillation Set-Up on Binary Classification

## Overview
This project sets up a simple knowledge distillation problem on a 2D binary classification problem using PyTorch.

There are 3 student training methods we compare:
1. Hard labels from teacher
2. Conditional mean of hard labels
3. Soft labesl from teacher (scaled with temperature)

## Dataset Formulation
We simulate a binary classification problem by generating a synthetic dataset of two Gaussian clusters.

* Class 0: $x \sim \mathcal{N}([-2, 0], I)$
* Class 1: $x \sim \mathcal{N}([2, 0], I)$

This dataset is linearly separable along the x-axis, which is essential for classification.

We then split this synthetic dataset into:
* 80% training
* 20% validaiton

## True Conditional Mean
For Method 2, we took the sigmoid of the hard labels generated by the teacher to provide a Bayes-optimal hard label. 

{
  1 if P(y=1|x) > 0.5
  0 otherwise
}

This acts like a "ground truth."

## Teacher Architecture
We modeled a large Multilayer Perceptron (MLP) neural network, which works great for classification tasks. 

* Linear(2, 64)
* ReLU - to introduce nonlinearity
* Linear(64, 64)
* ReLU
* Linear(64, 2) - output logits for 2 classes

## Student Architecture
We modeled a small MLP.

* Linear(2, 4)
* ReLU
* Linear(4, 2)

This ensures the student has much lower capacity than the teacher.

## Teacher Training Objective 
The teacher model is trained using cross entropy, common in classification tasks. 

We utilize the Adam optimzer, a learning rate of 1e-3, and 500 epochs.

## Distillation Targets
### Soft Targets
We utilize softmax on the teacher logits with a temperate T = 4 to soften the probability distribution.

This returns a probability of each class. 

### Hard Targets
We take the argmax of the highest probability of the teacher logits.

### Conditional Mean Hard Targets
We take the Bayes' conditional mean to choose the class with the highest probability. 

## Student Training
### Case 1 - Soft Targets
We use KL divergence to have the soft target probability distribution match that of the teacher distribution. 

### Case 2 - Hard Targets
We use cross-entropy (standardized supervised learning) using the teacher predictions. 

### Case 3 - Conditional Mean Hard Targets
We use cross-entropy (standardized supervised learning) using the optimal teacher predictions.

## Plot

We plot the training loss and validation loss for hard teacher labels, conditional hard labels, and soft teacher labels.

View TrainingVsValidationLoss.png for more details.

## Run the Code

Run the script:

```
python experiment.py
```

The training and validation curves will be displayed.
